{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mootha-sri-harshit/tic-tac-toe/blob/main/Copy_of_tic_tac_toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9  # 3x3 Board\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the board for a new game.\"\"\"\n",
        "        self.board = [' '] * 9\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Returns the board state as a tuple.\"\"\"\n",
        "        return tuple(self.board)\n",
        "\n",
        "    def available_moves(self):\n",
        "        \"\"\"Returns available moves (empty spaces).\"\"\"\n",
        "        return [i for i in range(9) if self.board[i] == ' ']\n",
        "\n",
        "    def make_move(self, position, player):\n",
        "        \"\"\"Makes a move if the position is valid.\"\"\"\n",
        "        if self.board[position] == ' ':\n",
        "            self.board[position] = player\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_winner(self):\n",
        "        \"\"\"Checks if there's a winner.\"\"\"\n",
        "        winning_combinations = [\n",
        "            (0, 1, 2), (3, 4, 5), (6, 7, 8),  # Rows\n",
        "            (0, 3, 6), (1, 4, 7), (2, 5, 8),  # Columns\n",
        "            (0, 4, 8), (2, 4, 6)  # Diagonals\n",
        "        ]\n",
        "        for (x, y, z) in winning_combinations:\n",
        "            if self.board[x] == self.board[y] == self.board[z] and self.board[x] != ' ':\n",
        "                return self.board[x]\n",
        "        if ' ' not in self.board:\n",
        "            return \"Draw\"\n",
        "        return None\n",
        "\n",
        "    def print_board(self):\n",
        "        \"\"\"Displays the board.\"\"\"\n",
        "        for i in range(0, 9, 3):\n",
        "            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n",
        "        print(\"-\" * 9)\n"
      ],
      "metadata": {
        "id": "u5YUyRPp3e8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Define the Deep Q-Network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.state_size = 9  # Tic-Tac-Toe board size\n",
        "        self.action_size = 9  # 9 possible moves\n",
        "        self.gamma = 0.95  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.memory = deque(maxlen=2000)  # Experience replay buffer\n",
        "\n",
        "        self.model = DQN(self.state_size, self.action_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def get_state_tensor(self, state):\n",
        "        \"\"\"Convert board state to tensor.\"\"\"\n",
        "        return torch.FloatTensor([1 if x == \"X\" else -1 if x == \"O\" else 0 for x in state])\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        \"\"\"Choose action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.choice(available_moves)\n",
        "        state_tensor = self.get_state_tensor(state).unsqueeze(0)\n",
        "        q_values = self.model(state_tensor)\n",
        "        sorted_moves = sorted(available_moves, key=lambda x: q_values[0][x].item(), reverse=True)\n",
        "        return sorted_moves[0]  # Best move from available options\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in memory for replay.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size=32):\n",
        "        \"\"\"Train the model using experience replay.\"\"\"\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                next_state_tensor = self.get_state_tensor(next_state).unsqueeze(0)\n",
        "                target += self.gamma * torch.max(self.model(next_state_tensor)).item()\n",
        "\n",
        "            state_tensor = self.get_state_tensor(state).unsqueeze(0)\n",
        "            target_tensor = self.model(state_tensor)\n",
        "            target_tensor[0][action] = target  # Update only the chosen action\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.criterion(self.model(state_tensor), target_tensor)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon for less exploration over time\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "XV89dEVy3wOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn_agent(episodes=10000):\n",
        "    env = TicTacToe()\n",
        "    agent = DQNAgent()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        player = \"X\"\n",
        "\n",
        "        while not done:\n",
        "            available_moves = env.available_moves()\n",
        "            action = agent.choose_action(state, available_moves)\n",
        "            env.make_move(action, player)\n",
        "\n",
        "            winner = env.check_winner()\n",
        "            next_state = env.get_state()\n",
        "\n",
        "            if winner == \"X\":\n",
        "                reward = 1\n",
        "                done = True\n",
        "            elif winner == \"O\":\n",
        "                reward = -1\n",
        "                done = True\n",
        "            elif winner == \"Draw\":\n",
        "                reward = 0\n",
        "                done = True\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "            agent.store_experience(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            player = \"O\" if player == \"X\" else \"X\"\n",
        "\n",
        "        agent.replay()\n",
        "\n",
        "        if episode % 1000 == 0:\n",
        "            print(f\"Training Episode {episode}, Epsilon: {agent.epsilon}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return agent\n",
        "\n",
        "# Train the DQN agent\n",
        "dqn_agent1 = train_dqn_agent()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8FX5TQUS3zqe",
        "outputId": "414b4dd2-b7ad-47ad-94a7-e71194766a33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Episode 0, Epsilon: 1.0\n",
            "Training Episode 1000, Epsilon: 0.00998645168764533\n",
            "Training Episode 2000, Epsilon: 0.00998645168764533\n",
            "Training Episode 3000, Epsilon: 0.00998645168764533\n",
            "Training Episode 4000, Epsilon: 0.00998645168764533\n",
            "Training Episode 5000, Epsilon: 0.00998645168764533\n",
            "Training Episode 6000, Epsilon: 0.00998645168764533\n",
            "Training Episode 7000, Epsilon: 0.00998645168764533\n",
            "Training Episode 8000, Epsilon: 0.00998645168764533\n",
            "Training Episode 9000, Epsilon: 0.00998645168764533\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to play against the AI\n",
        "def play_against_ai(agent):\n",
        "    env = TicTacToe()\n",
        "    state = env.reset()\n",
        "    player = \"X\"\n",
        "\n",
        "    print(\"Welcome to Tic-Tac-Toe AI!\")\n",
        "    env.print_board()\n",
        "\n",
        "    while True:\n",
        "        if player == \"X\":\n",
        "            try:\n",
        "                move = int(input(\"Enter your move (0-8): \"))\n",
        "                if move not in env.available_moves():\n",
        "                    print(\"Invalid move! Try again.\")\n",
        "                    continue\n",
        "            except ValueError:\n",
        "                print(\"Invalid input! Enter a number between 0-8.\")\n",
        "                continue\n",
        "        else:\n",
        "            move = agent.choose_action(state, env.available_moves())\n",
        "            print(f\"AI chooses: {move}\")\n",
        "\n",
        "        if env.make_move(move, player):\n",
        "            env.print_board()\n",
        "            winner = env.check_winner()\n",
        "            if winner:\n",
        "                print(f\"Game Over! Winner: {winner}\")\n",
        "                break\n",
        "            player = \"O\" if player == \"X\" else \"X\"\n",
        "            state = env.get_state()\n",
        "        else:\n",
        "            print(\"Invalid move! Try again.\")\n",
        "\n",
        "# Play against the trained DQN agent\n",
        "play_against_ai(dqn_agent1)"
      ],
      "metadata": {
        "id": "P-QOk8m5_anX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446056df-9432-479b-db22-872e09ba7413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to Tic-Tac-Toe AI!\n",
            "  |   |  \n",
            "  |   |  \n",
            "  |   |  \n",
            "---------\n",
            "Enter your move (0-8): 7\n",
            "  |   |  \n",
            "  |   |  \n",
            "  | X |  \n",
            "---------\n",
            "AI chooses: 1\n",
            "  | O |  \n",
            "  |   |  \n",
            "  | X |  \n",
            "---------\n",
            "Enter your move (0-8): 4\n",
            "  | O |  \n",
            "  | X |  \n",
            "  | X |  \n",
            "---------\n",
            "AI chooses: 5\n",
            "  | O |  \n",
            "  | X | O\n",
            "  | X |  \n",
            "---------\n",
            "Enter your move (0-8): 6\n",
            "  | O |  \n",
            "  | X | O\n",
            "X | X |  \n",
            "---------\n",
            "AI chooses: 8\n",
            "  | O |  \n",
            "  | X | O\n",
            "X | X | O\n",
            "---------\n",
            "Enter your move (0-8): 2\n",
            "  | O | X\n",
            "  | X | O\n",
            "X | X | O\n",
            "---------\n",
            "Game Over! Winner: X\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}